\documentclass[12pt]{report}
\usepackage{natbib}
\usepackage{url}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[super]{nth}
\usepackage{rotating}
\usepackage{tikz}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{caption}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage[page]{appendix}
\usepackage{verbatim}
\usepackage{minted}
\setcounter{secnumdepth}{4}

\usepackage{hyperref}
\usepackage[xindy]{glossaries} 
\hypersetup{
    colorlinks=true, % false: boxed links; true: colored links
    linkcolor=black, % color of internal links
    citecolor=black, % color of links to bibliography
    filecolor=black, % color of file links
    urlcolor=black % color of external links
}
%\renewcommand*{\glstextformat}[1]{\textcolor{black}{#1}}
\renewcommand{\glstextformat}[1]{\textit{#1}}
\definecolor{light-gray}{gray}{0.95}


\setmarginsrb{3 cm}{2.5 cm}{3 cm}{2.5 cm}{1 cm}{1.5 cm}{1 cm}{1.5 cm}
\title{ML Cheatsheet}								% Title
\author{Axel Mendoza}								% Author
\date{November 17, 2020}											% Date

\makeatletter
\let\thetitle\@title
\let\theauthor\@author
\let\thedate\@date
\makeatother

\pagestyle{fancy}
\fancyhf{}
\rhead{\theauthor}
\lhead{\thetitle}
\cfoot{\thepage}

\let\origdoublepage\cleardoublepage
\newcommand{\clearemptydoublepage}{%
  \clearpage
  {\pagestyle{empty}\origdoublepage}%
}

\input{glossary}
\makeglossaries
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{titlepage}
	\centering
    \includegraphics[width=0.51\linewidth]{images/epita_logo.png}\\[2.0 cm]	% University Logo
    \textsc{\Large Introducing}\\[0.4 cm]
    %\textsc{{\large Confidential Report}}\\[0.2 cm]				% Course Name
	\rule{\linewidth}{0.2 mm} \\[0.4 cm]
	{ \huge \bfseries \thetitle}\\
	\rule{\linewidth}{0.2 mm} \\[0.6 cm]
	
	\begin{minipage}{0.4\textwidth}
		\begin{flushleft} \large
			\emph{Author:}\\
			\theauthor
			\end{flushleft}
			\end{minipage}~
			\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
			\emph{College:} \\
            Epita 2018									% Your Student Number
		\end{flushright}
		
	\end{minipage}\\[0.6 cm]
	
	{\large \thedate}\\[2 cm]
    \includegraphics[width=0.45\linewidth]{images/58065.jpg}\\[2.0 cm]	% University Logo
    %\includegraphics[width=0.45\linewidth]{engie_lab.png}	% University Logo
	\vfill
	
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
\tableofcontents
}
\pagebreak

\renewcommand{\theFancyVerbLine}{
  \sffamily\textcolor[rgb]{0.5,0.5,0.5}{\scriptsize\arabic{FancyVerbLine}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Python}
    \section{Basics}
        \subsection{Read \& Write}
            Read csv with Pandas:
            \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    import pandas as pd
    df = pd.read_csv(csv_path, sep=',')
            \end{minted}
            
            Read excel with Pandas:
            \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    import pandas as pd
    df = pd.read_excel(data_path)
            \end{minted}
    \section{Training}
        Make train test split:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,
                                                        random_state=42)
        \end{minted}
        
        Standardize the data:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    from sklearn.preprocessing import StandardScaler
    X = StandardScaler().fit_transform(X)
        \end{minted}
        
        Print a classification report and a confusion matrix:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    from sklearn.metrics import classification_report
    from sklearn.metrics import confusion_matrix
    confusion_matrix(y_true, y_pred)
    classification_report(y_true, y_pred)
        \end{minted}
        
        Cross validation:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    from sklearn.model_selection import cross_val_score
    acc = cross_val_score(model, X, y, scoring='accuracy', cv=5).mean()
        \end{minted}
        
        Convert category to integer:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    from sklearn.preprocessing import LabelEncoder
    y = LabelEncoder().fit_transform(y)
        \end{minted}
        
        
        
    \section{Numpy}
        Common numpy operations:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
        np.vstack(arr_1, arr_2)         # Stack element in the vertical axis
        np.hstack(arr_1, arr_2)         # Stack element in the horizontal axis
        np.random.uniform(r_1, r_2, size)
        np.random.multivariate_normal(mean, cov, size)
        \end{minted}
        
    \section{PyTorch}
        Common tensor operations:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    import torch
    tsr = torch.ones((10, 20), dtype=torch.float32)
    tsr = torch.from_numpy(array)           # Convert a numpy array to a tensor
    tsr = torch.FloatTensor(list)           # Convert a list to a tensor
    tsr.unsqueeze(1)                        # Add a dimension to a tensor
    tsr.squeeze(0)                          # Remove a dimension to a tensor
    res = t_1.mm(t_2)                       # Matrix multiplication
    tsr = tsr.T                             # Perform the transpose of a tensor
    tsr = torch.cat([t_1, t_2], dim=1)      # Concatenate two tensors
    tsr = tsr.inverse()                     # Computes the inverse of a tensor
    tsr = tsr.abs()                         # Computes the absolute value
    tsr = tsr.sqrt()                        # Computes the squared root
    tsr = tsr.var()                         # Computes the variance
    tsr = tsr.std()                         # Computes the standard deviation
    tsr = tsr.mean()                        # Computes the mean
    tsr = torch.exp(tsr)                    # Computes the exponential of tensor
    tsr = torch.log(tsr)                    # Computes the logarithm of tensor
    tsr = torch.pow(2)                      # Computes the power of a tensor
    tsr = tsr.clone()                       # Creates a copy of a tensor
    tsr[tsr > 1] = 1                        # Set elements based on a condition
    tsr.bincount()                          # Returns the bincount of a tensor
    tsr.argmax()                            # Returns the argmax of a tensor
    tsr.max()                               # Returns the max value of a tensor
    tsr.unique()                            # Returns the unique values of a tensor
    torch.where(tsr > 0)                    # Returns elements based on a condition
                                            # as a tuple if 2D
    tsr.scatter_(dim=1, index, src)         # Replace values ad index by src
    torch.randint(0, 10, shape)             # Sample random int in a range
        \end{minted}
        
        Create a random tensor in a range:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    def randvec(r1, r2, shape):
        return (r1 - r2) * torch.rand(shape) + r2
        \end{minted}
        
        Covariance:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    def covariance(t_1, t_2):
        t_1 -= t_1.mean(axis=1, keepdim=True)
        t_2 -= t_2.mean(axis=1, keepdim=True)
        fact = 1 / (t_1.shape[0] - 1)
        return fact * t_1.T.mm(t_2)
        \end{minted}
    \section{Exploratory data analysis}
        Two distribution plots side by side:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    fig, (ax_1, ax_2) = plt.subplots(1, 2, figsize=(10, 4))
    sns.distplot(data[0], bins=50, hist=True, ax=ax_1)
    sns.distplot(data[1], bins=50, hist=True, ax=ax_2)
        \end{minted}
            
        Heatmap with annotations and squared cells:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    plt.figure(figsize=(16, 8))
    sns.heatmap(df.corr(), squared=True, annot=True, fmt='.2f')
        \end{minted}
        
        Plot points in 2D using scatter:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    plt.scatter(x, y)
    plt.xlabel('Title x')
    plt.ylabel('Title y')
        \end{minted}
        
        Pairwise plot to visualize the correlation of the features:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    pp = sns.pairplot(df, height=1.8, aspect=1.8,
                      plot_kws=dict(edgecolor='k', linewidth=0.5),
                      diag_kind='kde', diag_kws=dict(shade=True))
    pp.fig.subplots_adjust(top=0.93, wspace=0.3)
    
        \end{minted}
        
        Plot an histogram for each feature:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    df.hist(bins=14, color='steelblue', edgecolor='black', linewidth=1.0,
            xlabelsize=8, ylabelsize=8, grid=False)
    plt.tight_layout(rect=(0, 0, 1.2, 1.2))
        \end{minted}
        
        Set x and y labels and x, y range:
        
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    fig, ax = plt.subplots()
    ax.set_xlabel('X label', fontsize=12)
    ax.set_ylabel('Y label', fontsize=12)
    ax.set_xlim(-10, 35)
    ax.set_ylim(-10, 35)
        \end{minted}
        
        
        Count values of a discrete feature:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    sns.countplot(x='Column Name', data=df)
        \end{minted}
        
        Boxplot for ploting the relation between a discrete and a continuous feature:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    sns.boxplot(x='Discrete', y='Continuous', orient='h', data=df)
        \end{minted}
        
        Barplot for ploting the mean of each discrete feature:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    sns.barplot(x='Discete', y='Continuous', palette='hot', data=df)
        \end{minted}
        
        Plot a pie:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    def plot_pie(data, column='col_name', ax):
        counts = data[column].value_counts()
        percent = (counts * 100) / counts.sum()
        labels = counts.index
        ax.pie(x=percent, labels=labels, autopct='%1.0%%')
        \end{minted}
        
        
        DataFrame operations:
        \begin{minted}[linenos,frame=lines,framesep=2mm,bgcolor=light-gray,fontsize=\footnotesize]{python}
    df.describe()                               # Plot stats of each columns
    df = pd.concat([df_1, df_2], axis=0)        # Concatenate two DataFrame
    df['new_col'] = df['col_1'] + df['col_2']   # Create a new column
    df = df.drop(columns=['col_1', 'col_2'])    # Delete columns
    df.loc[df['col_1'] > 1, 'col_2']            # Retrive a column based on a 
                                                # condition over another column
    df.iloc[:, :-1]                             # Same loc but index based
                                                # instead of label based
        \end{minted}
\chapter{Machine Learning}
\chapter{Deep Learning}
    
        %A figure
        %\begin{figure}[ht]
        %    \centering
        %    \includegraphics[width=1\textwidth]{engie_lab.png}
        %    \caption{Example of human \gls{re-id} data labeling task}
        %\end{figure}
        %
        %Formula in line $\displaystyle \mathbf{w} \cdot \mathbf{x}$.
        %Formula with caption
        
        %\begin{equation}
        %H[n]=\begin{cases} 0, & n < 0, \\ 1, & n \ge 0, \end{cases}
        %\end{equation}
        %\subsection{Sub Section 1}\label{subsec1}
        %    %A glossary entry \gls{MLP}.
        %    %Two figures side by side.
        %    \begin{figure}[ht]
        %    \centering
        %    \begin{minipage}{.5\textwidth}
        %        \centering
        %        \includegraphics[width=1\textwidth]{epita_logo.png}
        %        \captionof{figure}{Convolution of stride 1}
        %        \label{fig:convstrideone}
        %    \end{minipage}%
        %    \begin{minipage}{.5\textwidth}
        %        \centering
        %        \includegraphics[width=1\textwidth]{engie_lab.png}
        %        \captionof{figure}{Convolution of stride 2}
        %        \label{fig:convstridetwo}
        %    \end{minipage}
        %    \end{figure}
        %    
        %    Itemize:
        %    \begin{itemize}
        %        \item Convolution layer for analyzing pattern in the image.
        %        \item Max Pooling layer to reduce the dimension of the input.
        %        \item Fully Connected layer to detect global configurations of the features extracted by the convolution layers.
        %        \item Softmax layer for classification.
        %    \end{itemize}
        %    
        %    Referencing the above Figure~\ref{fig:tripletnetwork}.
        %    \begin{figure}[ht]
        %        \centering
        %        \includegraphics[width=0.5\textwidth]{epita_logo.png}
        %        \caption{Offline Triplet Mining Schema.}
        %        \label{fig:tripletnetwork}
        %    \end{figure}
        %    
        %    Complex formula:
        %    \begin{equation}
        %    %\scalebox{1.00}{
        %        \begin{gathered}
        %            \displaystyle{w _ { i } ^ { + } = \left( d \left( \boldsymbol { f } _ { a }, \boldsymbol { f } _ { i } \right) + 1\right) ^ { \alpha }
        %            \quad ~~~~\text { if } \boldsymbol { f } _ { i } \in \boldsymbol { S } _ { a } ^ { + },} \\
        %            \displaystyle{w _ { j } ^ { - } = \left( d \left( \boldsymbol { f } _ { a } , \boldsymbol { f } _ { j } \right) + 1 \right) ^ { - 2 \alpha } \quad \text { if } \boldsymbol { f } _ { j } \in \boldsymbol { S } _ { a } ^ { - }}
        %        \end{gathered}%}
        %        \label{eq:batchhard}
        %    \end{equation}
        %    
        %    A nice table:
        %    \begin{figure}[ht]
        %        \centering
        %        \noindent\resizebox{\textwidth}{!}{%
        %        \begin{tabular}{|| l | c | c | c | l | c ||}
        %            \hline
        %            Method & mAP (\%) & Rank-1 & Rank-5 & Model & Year\\
        %            \hline
        %            \hline
        %            FACT+Plate+SNN+STR  & 27.77 & 61.44 & 78.78 & None & 2015 \\
        %            \hline
        %            OIFE-4-Views & 48.00 & 89.43 & ? & Custom & 2017 \\
        %            \hline
        %            VAMI & 50.13 & 77.03 & 90.82 & Custom & 2018 \\
        %            \hline
        %            Siamese-CNN+Path-LSTM  & 58.27 & 83.49 & 90.04 & RN50+LSTM & 2017 \\
        %            \hline
        %            Semihard+softmax+AIC  & 57.43 & 86.29 & 94.39 & RN50 & 2018\\
        %            \hline
        %            GAN+LSRO & 58.23 & 87.70 & 93.92 & RN50+DCGAN & 2018 \\
        %            \hline
        %            RAM-Multi-Learners  & 61.50 & 88.60 & 94.00 & VGG19x4 & 2018 \\
        %            \Xhline{4\arrayrulewidth}
        %            Baseline  ~~~~~~~~~~~~~~~~~($\boldsymbol{Ours}$)& 53.78 & 81.46 & 91.47 & RN50 & 2018\\
        %            \hline
        %            Hap2s+softmax \cite{spacetimeprior}+ ($\boldsymbol{Ours}$) & 56.85 & 84.80 & 92.43 & RN50 & 2018\\
        %            \hline
        %        \end{tabular}}
        %        \captionof{table}{Comparison with the related works on VeRi-776 }\label{fig:scores}
        %    \end{figure}
% - Limitation
\printglossaries

\bibliographystyle{plain}
\bibliography{biblist}
\end{document}